{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GPT-2 Finetuning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X02naVKmHQX9","executionInfo":{"status":"ok","timestamp":1620009246234,"user_tz":240,"elapsed":553,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"0c67b937-6bfe-4fed-badd-bfa0b0128d94"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JH0JtdCeE1n2","executionInfo":{"status":"ok","timestamp":1620009247385,"user_tz":240,"elapsed":803,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"90bc85d1-9383-4c69-dd38-c89599866d26"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon May  3 02:34:06 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    32W / 250W |   1437MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0E3KDIbR0tW5","executionInfo":{"status":"ok","timestamp":1620009250892,"user_tz":240,"elapsed":3661,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"a4315e50-e29d-4d1d-fd30-6b3439e2ae81"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPgBcQFS01YF","executionInfo":{"status":"ok","timestamp":1620009359593,"user_tz":240,"elapsed":112129,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"4d0480eb-10db-4729-8ce9-e9b0039efc4e"},"source":["!wget https://www.dropbox.com/s/xq4vosn9xyn1dy1/grouped_data.pickle?dl=0 -O grouped_data.pickle"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-03 02:34:10--  https://www.dropbox.com/s/xq4vosn9xyn1dy1/grouped_data.pickle?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6032:18::a27d:5212\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/xq4vosn9xyn1dy1/grouped_data.pickle [following]\n","--2021-05-03 02:34:10--  https://www.dropbox.com/s/raw/xq4vosn9xyn1dy1/grouped_data.pickle\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc700dc4761c648d31f4b23c1005.dl.dropboxusercontent.com/cd/0/inline/BNzQoVvBmap8trO3JSbT1C54QtHSLv4lxwh7AAdRQ1f2RLCpCYAOVfAQcgXQu8epqznMmhrNA8wKza0KjzQ_bczHu_Fdh9hjXbOCSBTZqwh9vp-ziM88KSVcuUh6OyPRKsGreCA3zdiSnp7Auh79jBsn/file# [following]\n","--2021-05-03 02:34:10--  https://uc700dc4761c648d31f4b23c1005.dl.dropboxusercontent.com/cd/0/inline/BNzQoVvBmap8trO3JSbT1C54QtHSLv4lxwh7AAdRQ1f2RLCpCYAOVfAQcgXQu8epqznMmhrNA8wKza0KjzQ_bczHu_Fdh9hjXbOCSBTZqwh9vp-ziM88KSVcuUh6OyPRKsGreCA3zdiSnp7Auh79jBsn/file\n","Resolving uc700dc4761c648d31f4b23c1005.dl.dropboxusercontent.com (uc700dc4761c648d31f4b23c1005.dl.dropboxusercontent.com)... 162.125.82.15, 2620:100:6032:15::a27d:520f\n","Connecting to uc700dc4761c648d31f4b23c1005.dl.dropboxusercontent.com (uc700dc4761c648d31f4b23c1005.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1740570798 (1.6G) [text/plain]\n","Saving to: â€˜grouped_data.pickleâ€™\n","\n","grouped_data.pickle 100%[===================>]   1.62G  17.0MB/s    in 1m 47s  \n","\n","2021-05-03 02:35:59 (15.5 MB/s) - â€˜grouped_data.pickleâ€™ saved [1740570798/1740570798]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0klQtoGm0-QG"},"source":["import pickle\n","import pandas as pd\n","import random\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","from transformers import TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments, AutoModelWithLMHead"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bk_GDnxi0-oq"},"source":["data = pd.read_pickle(r'grouped_data.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1RMp-0y1W46","executionInfo":{"status":"ok","timestamp":1620022495908,"user_tz":240,"elapsed":3614,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"5a3fcc71-98c6-4648-ff79-b3165f165876"},"source":["texts = []\n","bos = \"<BOS> \"\n","eos = \" <EOS> \"\n","\n","for key in sorted(list(data.keys())):\n","    post = data[key][0]\n","    comments = data[key][1]\n","    title = post['title']\n","    # try:\n","    #     text = post['selftext']\n","    # except:\n","    #     text = \"\"\n","    \n","    for comment in comments:\n","        if len(title) < 500 and len(comment['body']) < 500 and comment['body'] != '[deleted]' and comment['body'] != '[removed]':\n","            texts.append(title + bos + comment[\"body\"]  + eos )\n","\n","train_sample, test_sample = train_test_split(texts[280000*4:], test_size=0.25, shuffle=True, stratify=None)\n","\n","print(len(train_sample), len(test_sample))\n","\n","with open('train_text.txt', 'w', encoding='utf8') as f:\n","    for item in train_sample:\n","        f.write(\"%s\\n\" % item)\n","\n","with open('test_text.txt', 'w', encoding='utf8') as f:\n","    for item in test_sample:\n","        f.write(\"%s\\n\" % item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["212515 70839\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9gTLq-KdW0g","executionInfo":{"status":"ok","timestamp":1620022495909,"user_tz":240,"elapsed":1512,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"65e88d77-7fb9-410d-945e-1dc2ba489f2c"},"source":["len(texts)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1403354"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"VCaunLMtlPfw"},"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","train_path = 'train_text.txt'\n","test_path = 'test_text.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m9lHS0mIMak4","executionInfo":{"status":"ok","timestamp":1620022500903,"user_tz":240,"elapsed":2727,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"93f345b3-e7ac-400f-c73e-aab2a1d52137"},"source":["def load_dataset(train_path, test_path, tokenizer):\n","    train_dataset = TextDataset(\n","          tokenizer=tokenizer,\n","          file_path=train_path,\n","          block_size=128)\n","     \n","    test_dataset = TextDataset(\n","          tokenizer=tokenizer,\n","          file_path=test_path,\n","          block_size=128)   \n","    \n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=False,\n","    )\n","    return train_dataset, test_dataset, data_collator\n","\n","train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRy39rAy0mVm","executionInfo":{"status":"ok","timestamp":1620022500904,"user_tz":240,"elapsed":2338,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"09480a8c-4a8c-42d2-8b2f-db9fd92577de"},"source":["print('Size of training set:', len(train_dataset))\n","print('Size of test set:', len(test_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of training set: 92894\n","Size of test set: 30984\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ThG6jwL7qET8"},"source":["# Initialize `Trainer` with `TrainingArguments` and GPT-2 model\n","\n","TheÂ [Trainer](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer)Â class provides an API for feature-complete training. It is used in most of theÂ [example scripts](https://huggingface.co/transformers/examples.html) from Huggingface. Before we can instantiate our `Trainer` we need to download our GPT-2 model and create aÂ [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)Â to access all the points of customization during training. In the `TrainingArguments`, we can define the Hyperparameters we are going to use in the training process like our `learning_rate`, `num_train_epochs`, or  `per_device_train_batch_size`. A complete list can you find [here](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7hhmbT2ModI","executionInfo":{"status":"ok","timestamp":1620022551620,"user_tz":240,"elapsed":8210,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"916541ed-de8a-46b3-a082-3bd7b9483e05"},"source":["\n","\n","model = AutoModelWithLMHead.from_pretrained(\"/content/gdrive/My Drive/gpt_title_only_0.8\")\n","\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-CC\", #The output directory\n","    overwrite_output_dir=True, #overwrite the content of the output directory\n","    num_train_epochs=2, # number of training epochs\n","    per_device_train_batch_size=16, # batch size for training\n","    per_device_eval_batch_size=16,  # batch size for evaluation\n","    eval_steps = 400, # Number of update steps between two evaluations.\n","    save_steps=800, # after # steps model is saved \n","    warmup_steps=500,# number of warmup steps for learning rate scheduler,\n","    prediction_loss_only=True,\n","    )\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ZyQh0jlTqR6-"},"source":["# Train and save the model\n","\n","To train the model we can simply run `Trainer.train()`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":825},"id":"wjIzSWPTKzBf","executionInfo":{"status":"ok","timestamp":1620026623483,"user_tz":240,"elapsed":4071404,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"8fc7f08d-9d05-4ecc-8e45-e9942ae380eb"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='11612' max='11612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11612/11612 1:07:47, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.233400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.065300</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>2.095000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>2.111900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>2.132100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>2.149400</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>2.168500</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>2.191000</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>2.198000</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>2.216500</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>2.239000</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>2.199500</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>2.130600</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>2.166500</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>2.176000</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>2.192000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>2.216700</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>2.226000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>2.244100</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>2.259500</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>2.281300</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>2.294800</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>2.315800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=11612, training_loss=2.197008133591597, metrics={'train_runtime': 4067.9267, 'train_samples_per_second': 2.855, 'total_flos': 1.7755716901404672e+16, 'epoch': 2.0, 'init_mem_cpu_alloc_delta': 16384, 'init_mem_gpu_alloc_delta': 514529280, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -515387392, 'train_mem_gpu_alloc_delta': 1508786688, 'train_mem_cpu_peaked_delta': 516616192, 'train_mem_gpu_peaked_delta': 4969645056})"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"lHXqTv8kqxpJ"},"source":["After training is done you can save the model by calling `save_model()`. This will save the trained model to our `output_dir` from our `TrainingArguments`."]},{"cell_type":"code","metadata":{"id":"Q5quyGeMNdjE"},"source":["trainer.save_model('/content/gdrive/My Drive/gpt_title_only_1')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PGEiQ1mhOyNv"},"source":["# Test the model\n","\n","To test the model we are going to use another [highlight of the transformers library](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines) called `pipeline`. [Pipelines](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines) are objects that offer a simple API dedicated to several tasks, among others also `text-generation`"]},{"cell_type":"code","metadata":{"id":"Fh4Q1BR0OtDo"},"source":["from transformers import pipeline\n","\n","chef = pipeline('text-generation',model='/content/gdrive/My Drive/gpt_title_only_0.4', tokenizer='gpt2',config={'max_length':1500})\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"laDp891gO25V","executionInfo":{"status":"ok","timestamp":1620014209204,"user_tz":240,"elapsed":9157,"user":{"displayName":"Ryan Cooper","photoUrl":"","userId":"03930546846624815383"}},"outputId":"ef783997-ce05-4583-9b00-ac6abf045452"},"source":["chef(\"Will Sophia do good on her exam tomorrow? <BOS> \")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'Will Sophia do good on her exam tomorrow? <BOS> \\nSongs that get stuck in your head?<BOS> [Cinema by Gorillaz and the Black Keys.](https://www.youtube.com/'}]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"77HLWozaJFQF"},"source":[""]}]}